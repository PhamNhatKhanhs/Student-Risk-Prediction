# app.py
import streamlit as st
import pandas as pd
import joblib
import io # C·∫ßn thi·∫øt ƒë·ªÉ t·∫°o file CSV t·∫£i xu·ªëng trong b·ªô nh·ªõ

# --- L·ªÜNH STREAMLIT ƒê·∫¶U TI√äN ---
st.set_page_config(
    page_title="D·ª± ƒëo√°n Nguy c∆° H·ªçc t·∫≠p",
    page_icon="üéì",
    layout="wide"
)

# --- C·∫•u h√¨nh c√°c l·ª±a ch·ªçn cho categorical features ---
categorical_options = {
    'school': ['GP', 'MS'], 'sex': ['F', 'M'], 'address': ['U', 'R'], 
    'famsize': ['LE3', 'GT3'], 'Pstatus': ['T', 'A'], 
    'Mjob': ['teacher', 'health', 'services', 'at_home', 'other'],
    'Fjob': ['teacher', 'health', 'services', 'at_home', 'other'],
    'reason': ['home', 'reputation', 'course', 'other'],
    'guardian': ['mother', 'father', 'other'],
    'schoolsup': ['yes', 'no'], 'famsup': ['yes', 'no'], 'paid': ['yes', 'no'], 
    'activities': ['yes', 'no'], 'nursery': ['yes', 'no'], 'higher': ['yes', 'no'], 
    'internet': ['yes', 'no'], 'romantic': ['yes', 'no']
}

# --- T·∫£i m√¥ h√¨nh v√† danh s√°ch c·ªôt ---
MODEL_FILENAME = 'random_forest_pipeline.joblib' 
FEATURE_COLUMNS_FILENAME = 'feature_columns.joblib'

@st.cache_resource 
def load_model_and_cols():
    try:
        pipeline = joblib.load(MODEL_FILENAME)
        feature_cols = joblib.load(FEATURE_COLUMNS_FILENAME)
        return pipeline, feature_cols
    except FileNotFoundError:
        return None, None
    except Exception as e: 
        st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh ho·∫∑c file c·ªôt: {e}")
        return None, None

pipeline, feature_columns = load_model_and_cols()

# --- Ti√™u ƒë·ªÅ v√† Gi·ªõi thi·ªáu ---
st.title("üéì H·ªá th·ªëng D·ª± ƒëo√°n Nguy c∆° H·ªçc t·∫≠p c·ªßa H·ªçc sinh")
st.markdown("""
Ch√†o m·ª´ng! C√¥ng c·ª• n√†y d√πng Machine Learning (Random Forest) ƒë·ªÉ ∆∞·ªõc t√≠nh kh·∫£ nƒÉng h·ªçc sinh ƒë·∫°t k·∫øt qu·∫£ th·∫•p (G3 < 10).
B·∫°n c√≥ th·ªÉ nh·∫≠p th√¥ng tin cho t·ª´ng h·ªçc sinh ho·∫∑c t·∫£i l√™n file CSV ƒë·ªÉ d·ª± ƒëo√°n h√†ng lo·∫°t.
""")
st.markdown("---")

if pipeline is None or feature_columns is None:
    st.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file '{MODEL_FILENAME}' ho·∫∑c '{FEATURE_COLUMNS_FILENAME}'.")
    st.warning("Vui l√≤ng ch·∫°y `python main.py` ƒë·ªÉ hu·∫•n luy·ªán v√† l∆∞u m√¥ h√¨nh tr∆∞·ªõc.")
    st.stop()

# --- Tab cho nh·∫≠p li·ªáu th·ªß c√¥ng v√† t·∫£i file ---
tab1, tab2 = st.tabs(["üìù Nh·∫≠p li·ªáu Th·ªß c√¥ng", "üìÇ T·∫£i l√™n File CSV"])

with tab1:
    st.subheader("Nh·∫≠p th√¥ng tin cho m·ªôt h·ªçc sinh:")
    with st.form(key="student_info_form_manual"):
        input_data_manual = {}
        with st.expander("üí° H∆∞·ªõng d·∫´n v·ªÅ c√°c y·∫øu t·ªë ƒë·∫ßu v√†o (Nh·∫•n ƒë·ªÉ xem)", expanded=False):
            st.markdown("""
            * **G1, G2:** ƒêi·ªÉm gi·ªØa k·ª≥ 1 v√† gi·ªØa k·ª≥ 2 (0-20).
            * **absences:** S·ªë bu·ªïi v·∫Øng h·ªçc.
            * **studytime:** Th·ªùi gian h·ªçc h√†ng tu·∫ßn (1: <2 gi·ªù, 2: 2-5 gi·ªù, 3: 5-10 gi·ªù, 4: >10 gi·ªù).
            * **failures:** S·ªë l·∫ßn thi tr∆∞·ª£t c√°c m√¥n tr∆∞·ªõc ƒë√≥ (0-4).
            * *(V√† c√°c th√¥ng tin kh√°c v·ªÅ tr∆∞·ªùng, gia ƒë√¨nh, th√≥i quen...)*
            """)

        col1_manual, col2_manual, col3_manual = st.columns(3)
        cols_for_layout_manual = [col1_manual, col2_manual, col3_manual]
        
        for i, feature in enumerate(feature_columns):
            target_col_manual = cols_for_layout_manual[i % 3]
            help_text = None; caption_text = None
            
            if feature == 'Medu': caption_text = "H·ªçc v·∫•n m·∫π (0-4)"
            elif feature == 'Fedu': caption_text = "H·ªçc v·∫•n cha (0-4)"
            elif feature == 'studytime': help_text = "1: <2h, 2: 2-5h, 3: 5-10h, 4: >10h / tu·∫ßn"
            elif feature == 'failures': help_text = "S·ªë l·∫ßn thi tr∆∞·ª£t tr∆∞·ªõc (0-4)"
            elif feature in ['G1', 'G2']: help_text = "ƒêi·ªÉm (0-20)"

            if feature in categorical_options:
                input_data_manual[feature] = target_col_manual.selectbox(f"Ch·ªçn {feature}:", options=categorical_options[feature], key=f"manual_{feature}", help=help_text)
            elif feature in ['G1', 'G2', 'age', 'absences'] or feature in ['Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health']:
                default_val_map = {'age': 16, 'absences': 0, 'G1':10, 'G2':10, 'Medu':2, 'Fedu':2, 'traveltime':1, 'studytime':2, 'failures':0, 'famrel':3, 'freetime':3, 'goout':3, 'Dalc':1, 'Walc':1, 'health':3}
                min_val_map = {'age': 15, 'absences': 0, 'G1':0, 'G2':0, 'Medu':0, 'Fedu':0, 'traveltime':1, 'studytime':1, 'failures':0, 'famrel':1, 'freetime':1, 'goout':1, 'Dalc':1, 'Walc':1, 'health':1}
                max_val_map = {'age': 22, 'absences': 93, 'G1':20, 'G2':20, 'Medu':4, 'Fedu':4, 'traveltime':4, 'studytime':4, 'failures':4, 'famrel':5, 'freetime':5, 'goout':5, 'Dalc':5, 'Walc':5, 'health':5}
                
                input_data_manual[feature] = target_col_manual.number_input(
                    f"Nh·∫≠p {feature}:", 
                    min_value=min_val_map.get(feature,0), 
                    max_value=max_val_map.get(feature,20 if feature in ['G1','G2'] else (93 if feature=='absences' else (22 if feature=='age' else 5 ) ) ), # ƒêi·ªÅu ch·ªânh max value
                    value=default_val_map.get(feature,0), 
                    step=1, key=f"manual_{feature}", help=help_text
                )
            else: 
                input_data_manual[feature] = target_col_manual.number_input(f"Nh·∫≠p {feature} (kh√°c):", value=0, key=f"manual_{feature}", help=help_text)
            
            if caption_text: target_col_manual.caption(caption_text)

        submit_button_manual = st.form_submit_button(label="üöÄ D·ª± ƒëo√°n Nguy c∆° (Th·ªß c√¥ng)", use_container_width=True, type="primary")

    if submit_button_manual:
        ordered_input_data_manual = {col: input_data_manual[col] for col in feature_columns}
        input_df_manual = pd.DataFrame([ordered_input_data_manual])
        st.markdown("---")
        st.subheader("üìà K·∫øt qu·∫£ D·ª± ƒëo√°n (Th·ªß c√¥ng):")
        try:
            prediction_label = pipeline.predict(input_df_manual)[0]
            prediction_proba = pipeline.predict_proba(input_df_manual)[0]
            risk_status_str = "‚ö†Ô∏è Nguy c∆° cao (G3 < 10)" if prediction_label == 1 else "‚úÖ An to√†n (G3 >= 10)"
            proba_risk = prediction_proba[1]
            res_col1_manual, res_col2_manual = st.columns([2,3])
            with res_col1_manual:
                if prediction_label == 1: st.error(f"**Tr·∫°ng th√°i: {risk_status_str}**")
                else: st.success(f"**Tr·∫°ng th√°i: {risk_status_str}**")
            with res_col2_manual:
                st.metric(label="X√°c su·∫•t l√† 'Nguy c∆° cao'", value=f"{proba_risk:.2%}")
                st.progress(proba_risk)
            
            st.markdown("---")
            st.subheader("üìù L·ªùi khuy√™n:")
            if proba_risk > 0.7: st.warning("H·ªçc sinh n√†y c√≥ nguy c∆° **r·∫•t cao**. C·∫ßn quan t√¢m ƒë·∫∑c bi·ªát v√† h·ªó tr·ª£ c·ª• th·ªÉ.")
            elif proba_risk > 0.4: st.info("H·ªçc sinh n√†y c√≥ **d·∫•u hi·ªáu nguy c∆°**. N√™n theo d√µi v√† khuy·∫øn kh√≠ch th√™m.")
            else: st.balloons(); st.info("H·ªçc sinh n√†y c√≥ v·∫ª h·ªçc t·∫≠p t·ªët. Ti·∫øp t·ª•c duy tr√¨ v√† khuy·∫øn kh√≠ch!")
        except Exception as e:
            st.error(f"L·ªói d·ª± ƒëo√°n: {e}")

with tab2:
    st.subheader("T·∫£i l√™n file CSV ƒë·ªÉ d·ª± ƒëo√°n h√†ng lo·∫°t:")
    st.markdown("File CSV c·∫ßn c√≥ c√°c c·ªôt gi·ªëng nh∆∞ d·ªØ li·ªáu hu·∫•n luy·ªán v√† ƒë∆∞·ª£c ph√¢n t√°ch b·∫±ng d·∫•u ch·∫•m ph·∫©y (`;`).")
    
    uploaded_file = st.file_uploader("Ch·ªçn file CSV", type=["csv"], help="File ph·∫£i c√≥ c√°c c·ªôt: " + ", ".join(feature_columns[:5]) + "...") # Hi·ªÉn th·ªã 5 c·ªôt ƒë·∫ßu l√†m v√≠ d·ª•

    if uploaded_file is not None:
        try:
            # Quan tr·ªçng: ƒê·∫£m b·∫£o ƒë·ªçc file v·ªõi ƒë√∫ng separator
            new_data_df = pd.read_csv(uploaded_file, sep=';')
            st.success("ƒê√£ t·∫£i l√™n v√† ƒë·ªçc file CSV th√†nh c√¥ng!")
            st.write("Xem tr∆∞·ªõc 5 d√≤ng d·ªØ li·ªáu t·ª´ file ƒë√£ t·∫£i:", new_data_df.head())

            # --- KI·ªÇM TRA V√Ä CƒÇN CH·ªàNH C·ªòT ---
            # 1. Ki·ªÉm tra c√°c c·ªôt b·ªã thi·∫øu so v·ªõi feature_columns
            missing_cols = set(feature_columns) - set(new_data_df.columns)
            if missing_cols:
                st.error(f"L·ªói: File CSV t·∫£i l√™n b·ªã thi·∫øu c√°c c·ªôt b·∫Øt bu·ªôc sau: `{', '.join(missing_cols)}`")
                st.error(f"M√¥ h√¨nh c·∫ßn c√°c c·ªôt: `{', '.join(feature_columns)}`")
                st.stop() # D·ª´ng x·ª≠ l√Ω n·∫øu thi·∫øu c·ªôt

            # 2. Ch·ªçn ƒë√∫ng c√°c c·ªôt theo th·ª© t·ª± c·ªßa feature_columns, b·ªè qua c√°c c·ªôt th·ª´a
            # ƒêi·ªÅu n√†y c·ª±c k·ª≥ quan tr·ªçng ƒë·ªÉ pipeline ti·ªÅn x·ª≠ l√Ω ho·∫°t ƒë·ªông ƒë√∫ng
            try:
                df_to_predict = new_data_df[feature_columns].copy()
            except KeyError as e:
                st.error(f"L·ªói: Kh√¥ng th·ªÉ truy c·∫≠p m·ªôt ho·∫∑c nhi·ªÅu c·ªôt c·∫ßn thi·∫øt t·ª´ file CSV. ƒê·∫£m b·∫£o t√™n c·ªôt trong file CSV c·ªßa b·∫°n kh·ªõp ch√≠nh x√°c (ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng) v·ªõi c√°c c·ªôt m√† m√¥ h√¨nh mong ƒë·ª£i. L·ªói chi ti·∫øt: {e}")
                st.error(f"C√°c c·ªôt m√¥ h√¨nh mong ƒë·ª£i: `{', '.join(feature_columns)}`")
                st.error(f"C√°c c·ªôt c√≥ trong file b·∫°n t·∫£i l√™n: `{', '.join(new_data_df.columns.tolist())}`")
                st.stop()
            
            st.write("D·ªØ li·ªáu sau khi ch·ªçn v√† s·∫Øp x·∫øp c·ªôt (5 d√≤ng ƒë·∫ßu):", df_to_predict.head())

            if st.button("üìä Th·ª±c hi·ªán D·ª± ƒëo√°n cho To√†n b·ªô File CSV", use_container_width=True, type="primary"):
                with st.spinner("‚è≥ ƒêang x·ª≠ l√Ω v√† d·ª± ƒëo√°n... Vui l√≤ng ƒë·ª£i."):
                    predictions_batch = pipeline.predict(df_to_predict)
                    probabilities_batch = pipeline.predict_proba(df_to_predict)

                    # T·∫°o DataFrame k·∫øt qu·∫£
                    results_df_batch = df_to_predict.copy() 
                    results_df_batch['D·ª± ƒëo√°n Nguy c∆° (Nh√£n)'] = ["Nguy c∆° cao" if p == 1 else "An to√†n" for p in predictions_batch]
                    results_df_batch['X√°c su·∫•t Nguy c∆° cao'] = [p[1] for p in probabilities_batch] # L∆∞u d·∫°ng s·ªë ƒë·ªÉ d·ªÖ sort/filter
                
                st.success("üéâ Ho√†n th√†nh d·ª± ƒëo√°n cho to√†n b·ªô file!")
                st.subheader("K·∫øt qu·∫£ D·ª± ƒëo√°n H√†ng lo·∫°t:")
                st.dataframe(results_df_batch)

                # Cho ph√©p t·∫£i xu·ªëng k·∫øt qu·∫£
                # S·ª≠ d·ª•ng io.StringIO ƒë·ªÉ t·∫°o file CSV trong b·ªô nh·ªõ
                csv_buffer = io.StringIO()
                results_df_batch.to_csv(csv_buffer, sep=';', index=False, encoding='utf-8-sig') # utf-8-sig ƒë·ªÉ Excel ƒë·ªçc ti·∫øng Vi·ªát t·ªët
                
                st.download_button(
                    label="üì• T·∫£i xu·ªëng K·∫øt qu·∫£ D·ª± ƒëo√°n (CSV)",
                    data=csv_buffer.getvalue(),
                    file_name="predictions_student_risk_batch.csv",
                    mime="text/csv",
                    use_container_width=True
                )
        except pd.errors.ParserError:
            st.error("L·ªói ƒë·ªçc file CSV: C√≥ v·∫ª file kh√¥ng ƒë∆∞·ª£c ph√¢n t√°ch b·∫±ng d·∫•u ch·∫•m ph·∫©y (';') ho·∫∑c c√≥ l·ªói ƒë·ªãnh d·∫°ng kh√°c. Vui l√≤ng ki·ªÉm tra l·∫°i file.")
        except Exception as e:
            st.error(f"ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën khi x·ª≠ l√Ω file: {e}")


# --- Footer ---
st.markdown("---")
st.caption("Hackathon AI - 01/06/2025 | D·ª± √°n: D·ª± ƒëo√°n Nguy c∆° H·ªçc t·∫≠p c·ªßa H·ªçc sinh")